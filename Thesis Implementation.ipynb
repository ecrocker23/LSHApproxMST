{
 "cells": [
  {
   "cell_type": "raw",
   "id": "86003962",
   "metadata": {},
   "source": [
    "### Implementation of the algorithm by Har-Peled, Indyk, and Motwani in their paper titled \"Approximate Near Neighbor:\n",
    "###      Toward Removing the Curse of Dimensionality\" which approximates a minimum spanning tree.\n",
    "###\n",
    "### Elizabeth Crocker '23, advised by Deeparnab Chakrabarty, Associate Professor, \n",
    "###      Dartmouth College Department of Computer Science"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b89f2c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DISTANCE FUNCTION\n",
    "#\n",
    "# INPUT: points p and q, represented as lists of coordinates\n",
    "#\n",
    "# OUTPUT: the Hamming distance between p and q, or the number of coordinates which differ between them\n",
    "def D(p, q):\n",
    "    \n",
    "    d = len(p) # dimension\n",
    "    \n",
    "    if len(q) != d: # p and q are of different dimensions\n",
    "        return None\n",
    "    \n",
    "    dist = 0 # to track Hamming distance\n",
    "    for i in range(d): \n",
    "        if p[i] != q[i]: # p and q different at coordinate i\n",
    "            dist += 1 \n",
    "            \n",
    "    return dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "814356e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HASH FUNCTION\n",
    "#\n",
    "# INPUT: a point p, represented as a tuple of coordinates; a multi-set I, represented as a list\n",
    "#\n",
    "# OUTPUT: the output of the function g defined by I, represented as a tuple\n",
    "def g(p, I):\n",
    "    \n",
    "    g = []\n",
    "    for i in I:\n",
    "        g.append(p[i]) # where the coordinate of p at index i gives h_i(p) in the context of the Hamming metric\n",
    "        \n",
    "    return tuple(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cbf60e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# PREPROCESSING FUNCTION\n",
    "#\n",
    "# INPUT: a list P of d-dimension tuples representing the point set; parameters k and L denoting level of sensitivity\n",
    "#      and accuracy, respectively\n",
    "#\n",
    "# OUTPUT: a dictionary, buckets, which stores a pointer to each p in P in the buckets g_1(p),...,g_L(p);\n",
    "#      a list, G, storing the multi-sets I_1,...,I_L, represented as lists, denoting the respective functions\n",
    "#      g_1,...,g_L\n",
    "def preProcess(P, k, L):\n",
    "    \n",
    "    d = len(P[0]) # determine dimension (also the size of H in the context of the Hamming metric)\n",
    "    \n",
    "    buckets = {} # stores a pointer to each p in P in the buckets g_1(p),...,g_L(p)\n",
    "    G = [] # stores the multi-sets I_1,...,I_L denoting the respective functions g_1,...,g_L\n",
    "    \n",
    "    for j in range(L): # choose L functions g_1,...,g_L from G \"independently and uniformly at random\"\n",
    "        \n",
    "        I_j = [] \n",
    "        \n",
    "        # build I_j (the multi-set that determines g_j, a function which concatenates several hash functions from H)\n",
    "        for i in range(k): # choose k hash functions h_i_1,...h_i_k from H\n",
    "            I_j.append(random.randrange(d)) # pick indices i_1,...i_k at random\n",
    "            \n",
    "        G.append(I_j) # store multi-set in G for querying purposes\n",
    "\n",
    "        # push each point p to g_j(p) bucket\n",
    "        for p in P:\n",
    "            \n",
    "            g_j_p = g(p, I_j) # determine g_j(p)\n",
    "            \n",
    "            if g_j_p in buckets:\n",
    "                buckets[g_j_p].add(p) # add p to existing bucket\n",
    "            else:\n",
    "                buckets[g_j_p] = {p} # create new bucket containing p\n",
    "            \n",
    "    return buckets, G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab07409",
   "metadata": {},
   "outputs": [],
   "source": [
    "# QUERY FUNCTION\n",
    "#\n",
    "# INPUT: a point q, represented as a tuple; radius r2; a dictionary, buckets, which stores a pointer to each p in P \n",
    "#      in the buckets g_1(p),...,g_L(p), unless p has been deleted; a list, G, storing the multi-sets I_1,...,I_L, \n",
    "#      represented as lists, denoting the respective functions g_1,...,g_L\n",
    "#\n",
    "# OUTPUT: a point, p, such that D(p,q) <= r2 OR None, in which case we do not guarantee that such a point does not\n",
    "#      exist\n",
    "def query(q, r, buckets, G):\n",
    "    \n",
    "    L = len(G) # level of accuracy, i.e., the number of concatenated hash functions which map to each p in P\n",
    "    \n",
    "    count = 0 # keeps track of number of points examined\n",
    "    \n",
    "    # iterate through each bucket g_1(q),...,g_L(q)\n",
    "    for I_j in G:\n",
    "        g_j_q = g(q, I_j)\n",
    "\n",
    "        # carry out a brute-force search for a near-neighbor of q in the bucket if it exists\n",
    "        #      (defined as a point p such that D(p,q) <= r2)\n",
    "        if g_j_q in buckets: # Note: this should evaluate to True unless q is not in P\n",
    "\n",
    "            for p in buckets[g_j_q]:\n",
    "                \n",
    "                count += 1\n",
    "\n",
    "                # if there is any point p such that D(p,q) <= r2 then we return the point, else we return None \n",
    "                if D(p, q) <= r:\n",
    "                    return p\n",
    "\n",
    "                # \"as it is possible that the total number of points stored in those buckets is large, we \n",
    "                #      interrupt the search after examining the first 3L points (including duplicates)\"\"\n",
    "                if count >= 3*L:\n",
    "                    return None # no point p with D(p,q) <= r2 has been found\n",
    "    \n",
    "    return None # no point p with D(p,q) <= r2 has been found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db02b0a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DELETE FUNCTION\n",
    "#\n",
    "# INPUT: a vertex to be deleted, p, represented as a tuple; a dictionary, buckets, which stores a pointer to each\n",
    "#      p in P in the buckets g_1(p),...,g_L(p) unless p has been already deleted; a list, G, storing the multi-sets \n",
    "#      I_1,...,I_L, represented as lists, denoting the respective functions g_1,...,g_L\n",
    "#\n",
    "# OUTPUT: no return value; the buckets dictionary will be updated such that no pointer to p is stored in any bucket\n",
    "def delete(p, buckets, G):\n",
    "\n",
    "    for I_j in G:\n",
    "        g_j_p = g(p, I_j)         # for each bucket g_1(p),...,g_L(p)\n",
    "        buckets[g_j_p].discard(p)  # remove p from the bucket "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea29702",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXACT CONNECTED COMPONENTS FUNCTION - used for testing against ApproximateCC\n",
    "#\n",
    "# INPUT: a list P of n d-dimensional tuples representing the point set; radius r\n",
    "#\n",
    "# OUTPUT: a partitioning, \"partition\", of the points in P, represented as a list of sets of tuples; a list, E_prime,\n",
    "#      of the edges, represented as tuples, which form the connected components of the partitioning; two points p, q \n",
    "#      are said to be \"connected\" if D(p, q) <= r\n",
    "def BruteForceCC(P, r):\n",
    "    \n",
    "    P = P.copy()\n",
    "\n",
    "    partition = [] # the partitioning\n",
    "    E_prime = []   # the edge list\n",
    "\n",
    "    while len(P) != 0: # there are still points to input into the partitioning\n",
    "    \n",
    "        p = next(iter(P)) # select any p in P\n",
    "        P.remove(p)       # delete p from P\n",
    "        \n",
    "        S = {p}   # the set of points to be queried\n",
    "        C = set() # the next connected component to be computed\n",
    "\n",
    "        # repeat until the connected component C has been completed\n",
    "        while len(S) != 0:\n",
    "      \n",
    "            q = S.pop() # select and remove any q from S\n",
    "            C.add(q)    # add q to C\n",
    "\n",
    "            # check all points in P\n",
    "            for p_prime in P:\n",
    "                if D(q, p_prime) <= r:  # if \"connected\"\n",
    "                    E_prime.append((q, p_prime)) # add the edge (q, p') to E'\n",
    "                    P.remove(p_prime)            # delete p' from P\n",
    "                    S.add(p_prime)               # add p' to S\n",
    "\n",
    "        partition.append(C)  # add C to the partition\n",
    "  \n",
    "    return partition, E_prime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a6a4228",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "# APPROXIMATE CONNECTED COMPONENTS FUNCTION\n",
    "#\n",
    "# INPUT: a list P of n d-dimension tuples representing the point set; approximation factor c; radius r\n",
    "#\n",
    "# OUTPUT: a partitioning, \"partition\", of the points in P, represented as a list of sets of tuples; a list, E_prime,\n",
    "#      of the edges, represented as tuples, which form the connected components of the partitioning; two points p, q \n",
    "#      are said to be \"connected\" if D(p, q) <= r. The returned partitioning is a c-approximation of the exact\n",
    "#      connected components, i.e., two points p, q may be in the same component if D(p, q) <= c*r\n",
    "def ApproximateCC(P, c, r):\n",
    "    \n",
    "    P = P.copy()\n",
    "\n",
    "    # By Proposition 3.8:\n",
    "    # set p1 = 1 - r/d\n",
    "    # set p2 = 1 - rc/d\n",
    "    # The definition of the family H of hash functions is thus (r, rc, p1, p2)-sensitive\n",
    "    \n",
    "    d = len(P[0]) # dimension of each point\n",
    "    \n",
    "    p1 = 1 - (r / d)\n",
    "    p2 = 1 - (r*c / d)\n",
    "    \n",
    "    # By Proof of Theorem 3.4:\n",
    "    # set k = ceil(log(n, 1/p2))\n",
    "    # set L = n^rho/p1\n",
    "    # where rho = log(1/p1) / log(1/p2)\n",
    "    # The above gives us a failure probability (that of the query function falsely returning no neighbor)\n",
    "    #      of at most f = 1/3 + 1/e < 1for the LSH construction of the near-neighbor data structure\n",
    "\n",
    "    n = len(P) # the size of the point set\n",
    "\n",
    "    rho = math.log(1 / p1) / math.log(1 / p2)\n",
    "\n",
    "    k = math.ceil(math.log(n, 1 / p2))\n",
    "    L = int(n**rho / p1)\n",
    "    \n",
    "    r2 = c*r # used by the query function to report an approximate near-neighbor\n",
    "\n",
    "    # construct a near-neighbor data structure for P\n",
    "    buckets, G = preProcess(P, k, L)\n",
    "\n",
    "    partition = [] # the partitioning\n",
    "    E_prime = []   # the edge list\n",
    "\n",
    "    while len(P) != 0: # there are still points to input into the partitioning\n",
    "    \n",
    "        p = next(iter(P))  # select any p in P\n",
    "\n",
    "        # delete p from P and from the associated near-neighbor structure\n",
    "        P.remove(p)\n",
    "        delete(p, buckets, G)  \n",
    "\n",
    "        S = {p}   # the set of points to be queried\n",
    "        C = set() # the next connected component to be computed\n",
    "\n",
    "        # repeat until S = ø, i.e., connected component C has been completed\n",
    "        while len(S) != 0:\n",
    "            \n",
    "            q = S.pop() # select and remove any q from S\n",
    "            C.add(q)    # add q to C\n",
    "\n",
    "            # let p' be the answer to the query of the near-neighbor structure on q\n",
    "            p_prime = query(q, r2, buckets, G)\n",
    "\n",
    "            # repeat until p' is null\n",
    "            while p_prime is not None:\n",
    "\n",
    "                E_prime.append((q,p_prime))  # add the edge (q, p') to E'\n",
    "\n",
    "                # delete p' from P and the associated NearNbr structure\n",
    "                P.remove(p_prime)\n",
    "                delete(p_prime, buckets, G)\n",
    "\n",
    "                S.add(p_prime)  # add p' to S\n",
    "\n",
    "                p_prime = query(q, r2, buckets, G)  # choose new p'\n",
    "\n",
    "        partition.append(C)  # add C to the partitioning\n",
    "\n",
    "    return partition, E_prime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d555e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PRIM'S ALGORITHM\n",
    "#\n",
    "# INPUT: a list P of n d-dimensional tuples representing the point set\n",
    "#\n",
    "# OUTPUT: E, a dictionary matching each point p in P to the edge connecting p to its parent in the MST; edge_sum,\n",
    "#      a sum over all the weights of the edges in E\n",
    "def Prim(P):\n",
    "    \n",
    "    P = P.copy() # create copy so as to not overwrite P\n",
    "    \n",
    "    d = len(P[0]) # the dimension of each point in P\n",
    "    \n",
    "    C = [] # weights of the shortest edge from each remaining point in P to the tree\n",
    "    E = {} # shortest edge from each point to the tree, or the edge from each point to its parent once it has been\n",
    "           #      inserted into the tree\n",
    "    \n",
    "    for i in range(len(P)):\n",
    "        C.append(d + 1) # d is the greatest possible distance between two points\n",
    "        E[P[i]] = False # no edges have been examined yet\n",
    "            \n",
    "    edge_sum = 0\n",
    "\n",
    "    while (len(P) != 0): # while there are still points to examine\n",
    "        \n",
    "        i = C.index(min(C)) # select the point which has the shortest edge to the tree\n",
    "        wt = C.pop(i)\n",
    "        \n",
    "        if wt < d + 1: \n",
    "            edge_sum += wt # add the weight of the edge to the sum\n",
    "        \n",
    "        v = P.pop(i) # v is the newly added point to the tree\n",
    "                \n",
    "        for j in range(len(P)): # iterate over all remaining points\n",
    "            w = P[j]\n",
    "            \n",
    "            # if the edge between v and w is shorter than the shortest edge connecting w to other points in the tree\n",
    "            if D(v, w) < C[j]:\n",
    "                C[j] = D(v, w) # update the edge weight\n",
    "                E[w] = (v, w)  # update the shortest edge\n",
    "            \n",
    "    return E, edge_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268c8464",
   "metadata": {},
   "outputs": [],
   "source": [
    "# APPROXIMATE MST ALGORITHM\n",
    "#\n",
    "# INPUT: a list P of n d-dimensional tuples representing the point set; approximation factors c, gamma\n",
    "#\n",
    "# OUTPUT: components, a dictionary mapping each point to its respective component in the partitioning as the tree \n",
    "#      is built, represented as a set; partitioning, a list of sets of points representing the connected components \n",
    "#      in the graph; E, a set of edges making up the tree; edge_sum, a sum over the weights of all edges in E\n",
    "def ApproximateMST(P, c, gamma):\n",
    "    \n",
    "    P = P.copy() # create copy so as to not overwrite P\n",
    "    \n",
    "    E = set() # the edge set\n",
    "    \n",
    "    d = len(P[0]) # the dimension of each point\n",
    "    r = d         # the maximum value of D(p, q) for any two points p, q is d\n",
    "            \n",
    "    n = len(P) # the number of points\n",
    "    M = int(math.log(n / gamma, 1 + gamma)) # the number of radii to be considered\n",
    "    \n",
    "    R = [] # list of radii\n",
    "    for i in range(M + 1): # for i = 0,...,M\n",
    "        r_i = r / math.pow(1 + gamma, M - i)\n",
    "        if r_i*c >= d: # instead of padding points with dummy zeros to increase dimension, we ensure here that p2 > 0\n",
    "            R.append((d - 1) / c)\n",
    "            break\n",
    "        elif r_i*c >= 1:\n",
    "            R.append(r_i) # for the Hamming metric only, r < 1 is pointless to consider with distinct points\n",
    "            \n",
    "    components = dict() # partitioning dictionary which maps each point to its respective component\n",
    "    partitioning = []   # list of the components in the partitioning\n",
    "    edge_sum = 0        # the sum over all edges in the tree\n",
    "    \n",
    "    for p in P: # initially, each point has its own component in the partiitoning\n",
    "        components[p] = {p}\n",
    "        partitioning.append({p})\n",
    "                \n",
    "    for i in range(len(R)): # for i = 0,...,M do\n",
    "        partitioning_i, E_prime = ApproximateCC(P, c, R[i]) # invoke ApproximateCC(P,c,r_i)\n",
    "\n",
    "        for e in E_prime: # for each edge (u, v) in E' do\n",
    "            \n",
    "            u, v = e\n",
    "            \n",
    "            P_i = components[u] \n",
    "            P_j = components[v]\n",
    "                                \n",
    "            if P_i != P_j: # if u and v belong to different sets P_i, P_j in the partitioning then\n",
    "                \n",
    "                partitioning.remove(P_i)\n",
    "                partitioning.remove(P_j)\n",
    "                \n",
    "                P_new = P_i.union(P_j)\n",
    "                \n",
    "                # merge P_i and P_j in the partitioning\n",
    "                for p in P_new:\n",
    "                    components[p] = P_new\n",
    "                partitioning.append(P_new)\n",
    "                \n",
    "                E.add(e) # add e to the edge set\n",
    "                edge_sum += D(u, v)  # add the edge sum\n",
    "\n",
    "    if len(partitioning) != 1: # in the case that the graph is not connected at the end of the algorithm\n",
    "        \n",
    "        P_random = []\n",
    "        \n",
    "        for comp in partitioning: # select a random point from each component of the partitioning\n",
    "            P_random.append(random.choice(list(comp)))\n",
    "            \n",
    "        addl_E, addl_sum = Prim(P_random) # run Prim's algorithm on the random points\n",
    "        \n",
    "        for p in P_random:\n",
    "            \n",
    "            if addl_E[p]: # i.e., not the root\n",
    "                \n",
    "                u, v = addl_E[p]\n",
    "            \n",
    "                P_i = components[u] \n",
    "                P_j = components[v]\n",
    "\n",
    "                if P_i != P_j: # if u and v belong to different sets P_i, P_j in the partitioning then\n",
    "\n",
    "                    partitioning.remove(P_i)\n",
    "                    partitioning.remove(P_j)\n",
    "\n",
    "                    P_new = P_i.union(P_j)\n",
    "\n",
    "                    # merge P_i and P_j in the partitioning\n",
    "                    for p in P_new:\n",
    "                        components[p] = P_new\n",
    "                    partitioning.append(P_new)\n",
    "\n",
    "                    E.add((u, v)) # add (u, v) to the edge set\n",
    "                    edge_sum += D(u, v)  # add the edge sum\n",
    "        \n",
    "    return components, partitioning, E, edge_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc7fc5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TREE CHECKER FUNCTION\n",
    "#\n",
    "# INPUT: n, the number of points in a graph; partitioning, a list of sets of points representing the connected\n",
    "#      components of the graph; E, a list representing the edge set of the graph\n",
    "#\n",
    "# OUTPUT: True, if the provided graph is a tree, and False otherwise; relies on graph theory that a connected graph\n",
    "#      with n-1 edges is acyclic and hence is a tree\n",
    "def isTree(n, partitioning, E):\n",
    "    \n",
    "    if len(partitioning) != 1: # graph is not connected (too many components)\n",
    "        print(\"fail a\")\n",
    "        return False\n",
    "        \n",
    "    if len(partitioning[0]) != n: # graph is not connected (missing vertices)\n",
    "        print(\"fail b\")\n",
    "        return False\n",
    "    \n",
    "    if len(E) != n-1: # graph is either not connected (too few edges) or is cyclic (too many edges)\n",
    "        print(\"fail c\")\n",
    "        return False\n",
    "    \n",
    "    for e in E:\n",
    "        (u, v) = e\n",
    "        if (v, u) in E: # graph is not connected (duplicate edges in set leading to too few edges)\n",
    "            print(\"fail d\")\n",
    "            return False\n",
    "        \n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b74627",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RANDOM POINT SET GENERATION FUNCTION #1: \"UNCLUSTERED\"\n",
    "#\n",
    "# INPUT: sigma, a list representation of the alphabet from which to choose point coordinates; d, the dimension of \n",
    "#     each point; n, the number of points to be generated\n",
    "#\n",
    "# OUTPUT: a list P of n d-dimensional tuples, with each coordinate of each point chosen independently and uniformly\n",
    "#      at random from sigma, with the exception of the first coordinate, which is equal to the index of the point in\n",
    "#      P to ensure points are distinct (i.e., have a Hamming distance >= 1)\n",
    "def unclusteredPointSet(sigma, d, n):\n",
    "    \n",
    "    P = [] # the list of points\n",
    "    \n",
    "    for i in range(n): # n points to be generated\n",
    "        \n",
    "        p = [] # list representation of the point\n",
    "        \n",
    "        p.append(i) # ensure points are distinct (i.e., Hamming metric >= 1) by setting first coordinate equal to i\n",
    "        \n",
    "        for j in range(d - 1): # points are d-dimensional\n",
    "            p.append(random.choice(sigma)) # each coordinate chosen independently and uniformly at random from sigma\n",
    "            \n",
    "        P.append(tuple(p)) # cast p as a tuple and add it to P\n",
    "    \n",
    "    return P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbdc3f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RANDOM POINT SET GENERATION FUNCTION #2: \"PROBABILITY CLUSTERS\"\n",
    "#\n",
    "# INPUT: d, the dimension of each point; n, the number of points to be generated\n",
    "#\n",
    "# OUTPUT: a list P of n d-dimensional tuples, each in one of 11 clusters—coordinates of points in the 0,...,10th  \n",
    "#      cluster are chosen independently at random with a 0,...,10/10 probability of being 0 and 10,...,0/10 \n",
    "#      probability of being 1, respectively, with the exception of the first coordinate, which is equal to the index\n",
    "#      of the point in P to ensure points are distinct (i.e., have a Hamming distance >= 1)\n",
    "def probClustersPointSet(d, n):\n",
    "    \n",
    "    P = [] # the list of points\n",
    "    \n",
    "    for i in range(n): # n points to be generated\n",
    "        \n",
    "        cluster = i // (n // 11) # cluster index representing the probability (out of 10) of choosing a 0 over a 1\n",
    "        \n",
    "        p = [] # list representation of the point\n",
    "        \n",
    "        p.append(i) # ensure points are distinct (i.e., Hamming metric >= 1) by setting first coordinate equal to i\n",
    "        \n",
    "        for j in range(d - 1): # points are d-dimensional\n",
    "            \n",
    "            pick = random.choice(range(11)) \n",
    "            if pick < cluster: \n",
    "                p.append(0) # select 0 with cluster/10 probability\n",
    "            else:\n",
    "                p.append(1) # select 1 with 1 - cluster/10 probability\n",
    "    \n",
    "        P.append(tuple(p)) # cast p as a tuple and add it to P\n",
    "\n",
    "    return P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc0cee65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RANDOM POINT SET GENERATION FUNCTION #3: \"MATCHED CLUSTERS\"\n",
    "#\n",
    "# INPUT: d, the dimension of each point; n, the number of points to be generated\n",
    "#\n",
    "# OUTPUT: a list P of n d-dimensional tuples, each in one of len(sigma) clusters—the first 20% of coordinates of each\n",
    "#      point are chosen independently and uniformly at random from sigma, with the exception of the first coordinate,\n",
    "#      which is equal to the index of the point in P to ensure points are distinct (i.e., have a Hamming distance \n",
    "#      >= 1); the remaining 80% of coordinates correspond to the element of sigma associated with the cluster;\n",
    "#      points within a cluster have a Hamming distance of at most d/5 and points in different clusters have a Hamming\n",
    "#      distance of at least 4d/5\n",
    "def matchedClustersPointSet(sigma, d, n):\n",
    "    \n",
    "    P = [] # the list of points\n",
    "    \n",
    "    for i in range(n): # n points to be generated\n",
    "        \n",
    "        k = i // (n // len(sigma)) # cluster index representing the element from sigma making up 80% of coordinates\n",
    "        \n",
    "        p = [] # list representation of the point\n",
    "        \n",
    "        p.append(i) # ensure points are distinct (i.e., Hamming metric >= 1) by setting first coordinate equal to i\n",
    "        \n",
    "        for j in range(1, d): # points are d-dimensional \n",
    "            \n",
    "            if j < d / 5: # first 20% of coordinates chosen independently and uniformly at random from sigma\n",
    "                p.append(random.choice(sigma))\n",
    "                \n",
    "            else: # remaining 80% of coordinates correspond to the element of sigma associated with the cluster\n",
    "                p.append(sigma[k-1])\n",
    "                \n",
    "        P.append(tuple(p)) # cast p as a tuple and add it to P\n",
    "            \n",
    "    return P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2480bae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD PLANTS TEST DATA\n",
    "import csv\n",
    "\n",
    "csv_filename = '/Users/elizacrocker/Desktop/Thesis/data/plants_data.csv' # replace with correct pathname\n",
    "\n",
    "with open(csv_filename) as file:\n",
    "    reader = csv.reader(file)\n",
    "    next(reader) # skip header line\n",
    "    plants = list(tuple(line) for line in reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a5f8957",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# TESTING PLANTS DATA\n",
    "\n",
    "c = [10, 20, 50, 100]\n",
    "gamma = 1\n",
    "\n",
    "for i in range(len(c)):\n",
    "    for j in range(5): # 5 trials for each c value\n",
    "        a_time = time.time()\n",
    "        comp, part, E_approx, approx_edge_sum = ApproximateMST(plants, c[i], gamma)\n",
    "        a_time = time.time() - a_time\n",
    "\n",
    "        print(isTree(len(plants), part, E_approx))\n",
    "        print(len(plants), \"c =\", c[i], \"approx:\", approx_edge_sum, a_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e85d94d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## TEST ENVIRONMENT\n",
    "\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "theoretical = []\n",
    "\n",
    "approx_times = []\n",
    "prim_times = []\n",
    "\n",
    "approx_sums = []\n",
    "exact_sums = []\n",
    "\n",
    "d = 100       # dimension\n",
    "gamma = 1     # approximation factor\n",
    "sigma = [0,1] # alphabet\n",
    "#n = 500\n",
    "\n",
    "for n_div in range(1, 11):\n",
    "    n = 10*n_div\n",
    "    \n",
    "    approx_times_2 = []\n",
    "    approx_sums_2 = []\n",
    "    \n",
    "    approx_times_3 = []\n",
    "    approx_sums_3 = []\n",
    "    \n",
    "    approx_times_4 = []\n",
    "    approx_sums_4 = []\n",
    "    \n",
    "    approx_times_10 = []\n",
    "    approx_sums_10 = []\n",
    "    \n",
    "    prim_times = []\n",
    "    exact_sums = []\n",
    "    \n",
    "    for i in range(5): # repeat each c value 5 times\n",
    "\n",
    "        # choose point set generation function\n",
    "#         P = unclusteredPointSet(sigma, d, n)\n",
    "#         P = probClustersPointSet(d, n)\n",
    "        P = matchedClustersPointSet(sigma, d, n)\n",
    "    \n",
    "        ##### TEST C = 2 #####\n",
    "        a_time = time.time()\n",
    "        comp, part, E_approx, approx_edge_sum = ApproximateMST(P, 2, gamma)\n",
    "        a_time = time.time() - a_time\n",
    "\n",
    "        approx_sums_2.append(approx_edge_sum)\n",
    "        approx_times_2.append(a_time)\n",
    "\n",
    "        if not isTree(n, part, E_approx):\n",
    "            print(n, c)\n",
    "            print(len(P))\n",
    "            print(len(part))\n",
    "            print(len(E_approx))\n",
    "           \n",
    "        ##### TEST C = 3 #####\n",
    "        a_time = time.time()\n",
    "        comp, part, E_approx, approx_edge_sum = ApproximateMST(P, 3, gamma)\n",
    "        a_time = time.time() - a_time\n",
    "\n",
    "        approx_sums_3.append(approx_edge_sum)\n",
    "        approx_times_3.append(a_time)\n",
    "\n",
    "        if not isTree(n, part, E_approx):\n",
    "            print(n, c)\n",
    "            print(len(P))\n",
    "            print(len(part))\n",
    "            print(len(E_approx))\n",
    "            \n",
    "        ##### TEST C = 4 #####\n",
    "        a_time = time.time()\n",
    "        comp, part, E_approx, approx_edge_sum = ApproximateMST(P, 4, gamma)\n",
    "        a_time = time.time() - a_time\n",
    "\n",
    "        approx_sums_4.append(approx_edge_sum)\n",
    "        approx_times_4.append(a_time)\n",
    "\n",
    "        if not isTree(n, part, E_approx):\n",
    "            print(n, c)\n",
    "            print(len(P))\n",
    "            print(len(part))\n",
    "            print(len(E_approx))\n",
    " \n",
    "        ##### TEST C = 10 #####\n",
    "        a_time = time.time()\n",
    "        comp, part, E_approx, approx_edge_sum = ApproximateMST(P, 10, gamma)\n",
    "        a_time = time.time() - a_time\n",
    "\n",
    "        approx_sums_10.append(approx_edge_sum)\n",
    "        approx_times_10.append(a_time)\n",
    "\n",
    "        if not isTree(n, part, E_approx):\n",
    "            print(n, c)\n",
    "            print(len(P))\n",
    "            print(len(part))\n",
    "            print(len(E_approx))\n",
    "              \n",
    "        ##### TEST PRIM #####\n",
    "        p_time = time.time()\n",
    "        E_prim, prim_edge_sum = Prim(P)\n",
    "        p_time = time.time() - p_time\n",
    "\n",
    "        exact_sums.append(prim_edge_sum)\n",
    "        prim_times.append(p_time)\n",
    "        \n",
    "    print(n)\n",
    "    print(\"approx sums, c = 2\")\n",
    "    for s in approx_sums_2:\n",
    "        print(s)\n",
    "\n",
    "    print(\"approx times, c = 2\")\n",
    "    for s in approx_times_2:\n",
    "        print(s)\n",
    "        \n",
    "    print(\"approx sums, c = 3\")\n",
    "    for s in approx_sums_3:\n",
    "        print(s)\n",
    "\n",
    "    print(\"approx times, c = 3\")\n",
    "    for s in approx_times_3:\n",
    "        print(s)\n",
    "        \n",
    "    print(\"approx sums, c = 4\")\n",
    "    for s in approx_sums_4:\n",
    "        print(s)\n",
    "\n",
    "    print(\"approx times, c = 4\")\n",
    "    for s in approx_times_4:\n",
    "        print(s)\n",
    "\n",
    "    print(\"approx sums, c = 10\")\n",
    "    for s in approx_sums_10:\n",
    "        print(s)\n",
    "\n",
    "    print(\"approx times, c = 10\")\n",
    "    for s in approx_times_10:\n",
    "        print(s)\n",
    "\n",
    "    print(\"exact sums\")\n",
    "    for s in exact_sums:\n",
    "        print(s)\n",
    "\n",
    "    print(\"prim times\")\n",
    "    for s in prim_times:\n",
    "        print(s)\n",
    "        \n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ffef65e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### THIS CODE TESTED TAKING IN THE EXISTING PARTITIONING AND ADDING CONNECTED COMPONENTS TO THE SET OF QUERY POINTS IMMEDIATELY\n",
    "\n",
    "import math\n",
    "\n",
    "def ApproximateCC2(P, partitions, c, r):\n",
    "\n",
    "    # By Proposition 3.8:\n",
    "    # set p1 = 1 - r/d\n",
    "    # set p2 = 1 - rc/d\n",
    "    d = len(P[0])\n",
    "    p1 = 1 - (r / d)\n",
    "    p2 = 1 - (r*c / d)\n",
    "\n",
    "\n",
    "    # By Proof of Theorem 3.4:\n",
    "    # set k = ceil(log(n, 1/p2))\n",
    "    # set L = n^rho/p1\n",
    "    # where rho = log(1/p1) / log(1/p2)\n",
    "\n",
    "    n = len(P)\n",
    "\n",
    "    arg1 = 1 / p1\n",
    "    arg2 = 1 / p2\n",
    "\n",
    "    rho = math.log(arg1) / math.log(arg2)\n",
    "\n",
    "    k = math.ceil(math.log(n, arg2))\n",
    "    L = int(n**rho / p1)\n",
    "    \n",
    "    r2 = c*r\n",
    "\n",
    "    # construct a NearNbr(P,c,r,f) data structure for P\n",
    "    # NOTE: the LSH construction of the NearNbr data structure gives us a\n",
    "    # failure probability of f <= 1/3 + 1/e < 1\n",
    "    buckets, G = Hash(P, k, L)\n",
    "    \n",
    "    partition = []\n",
    "    E_prime = []\n",
    "\n",
    "    while len(P) != 0:\n",
    "    \n",
    "        p = next(iter(P))  # select any p in P\n",
    "\n",
    "        # delete p and already connected components from P and from the associated NearNbr structure\n",
    "        S = {p}\n",
    "        P.remove(p)\n",
    "        delete(p, buckets, G)\n",
    "                \n",
    "        for el in partitions[p]:\n",
    "            if el in P:\n",
    "                S.add(el)\n",
    "                P.remove(el)\n",
    "                delete(el, buckets, G)\n",
    "        C = set() # the next connected component to be computed\n",
    "\n",
    "        # repeat until S = ø, i.e., connected component C has been completed\n",
    "        while len(S) != 0:\n",
    "\n",
    "            q = S.pop()     # select and remove any q from S\n",
    "            C.add(q)        # add q to C\n",
    "\n",
    "            # let p' be the answer of the NearNbr structure on q\n",
    "            p_prime = query(q, r2, buckets, G)\n",
    "\n",
    "            # repeat until p' is null\n",
    "            while p_prime is not None:\n",
    "\n",
    "                E_prime.append((q,p_prime))  # add {q, p'} to E'\n",
    "\n",
    "                # delete p' from P and the associated NearNbr structure\n",
    "                P.remove(p_prime)\n",
    "                delete(p_prime, buckets, G)\n",
    "\n",
    "                S.add(p_prime)  # add p' to S\n",
    "                \n",
    "                for el in partitions[p_prime]:\n",
    "                    if el in P:\n",
    "                        S.add(el)\n",
    "                        P.remove(el)\n",
    "                        delete(el, buckets, G)\n",
    "\n",
    "                p_prime = query(q, r2, buckets, G)  # choose new p'\n",
    "\n",
    "        partition.append(C)  # add C to the partition\n",
    "\n",
    "    return partition, E_prime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "653fb2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "### THIS CODE TESTED CHOOSING A POINT FROM EACH COMPONENT OF THE EXISTING PARTITIONING AT RANDOM TO QUERY\n",
    "\n",
    "import math\n",
    "\n",
    "def ApproximateCC3(P, p_list, p_map, c, r):\n",
    "\n",
    "    # By Proposition 3.8:\n",
    "    # set p1 = 1 - r/d\n",
    "    # set p2 = 1 - rc/d\n",
    "    d = len(P[0])\n",
    "    p1 = 1 - (r / d)\n",
    "    p2 = 1 - (r*c / d)\n",
    "\n",
    "\n",
    "    # By Proof of Theorem 3.4:\n",
    "    # set k = ceil(log(n, 1/p2))\n",
    "    # set L = n^rho/p1\n",
    "    # where rho = log(1/p1) / log(1/p2)\n",
    "\n",
    "    n = len(P)\n",
    "\n",
    "    arg1 = 1 / p1\n",
    "    arg2 = 1 / p2\n",
    "\n",
    "    rho = math.log(arg1) / math.log(arg2)\n",
    "\n",
    "    k = math.ceil(math.log(n, arg2))\n",
    "    L = int(n**rho / p1)\n",
    "    \n",
    "    r2 = c*r\n",
    "\n",
    "    # construct a NearNbr(P,c,r,f) data structure for P\n",
    "    # NOTE: the LSH construction of the NearNbr data structure gives us a\n",
    "    # failure probability of f <= 1/3 + 1/e < 1\n",
    "    buckets, G = Hash(P, k, L)\n",
    "\n",
    "    partition = []\n",
    "    E_prime = []\n",
    "\n",
    "    while len(p_list) != 0:\n",
    "    \n",
    "        part = next(iter(p_list))  # select any p in P\n",
    "        p = random.choice(list(part))\n",
    "\n",
    "        # delete p and already connected components from P and from the associated NearNbr structure\n",
    "        S = {p}\n",
    "                \n",
    "        for el in part:\n",
    "            delete(el, buckets, G)\n",
    "                \n",
    "        p_list.remove(part)\n",
    "                \n",
    "        C = set() # the next connected component to be computed\n",
    "\n",
    "        # repeat until S = ø, i.e., connected component C has been completed\n",
    "        while len(S) != 0:\n",
    "            \n",
    "            q = S.pop()     # select and remove any q from S\n",
    "            C.update(p_map[q])        # add q to C\n",
    "\n",
    "            # let p' be the answer of the NearNbr structure on q\n",
    "            p_prime = query(q, r2, buckets, G)\n",
    "\n",
    "            # repeat until p' is null\n",
    "            while p_prime is not None:\n",
    "\n",
    "                E_prime.append((q,p_prime))  # add {q, p'} to E'\n",
    "\n",
    "                S.add(p_prime)  # add p' to S\n",
    "                \n",
    "                P_p_prime = p_map[p_prime]\n",
    "                \n",
    "                for el in P_p_prime:\n",
    "                    delete(el, buckets, G)\n",
    "                        \n",
    "                p_list.remove(P_p_prime)\n",
    "                \n",
    "                p_prime = query(q, r2, buckets, G)  # choose new p'\n",
    "\n",
    "        partition.append(C)  # add C to the partition\n",
    "\n",
    "    return partition, E_prime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e73523c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "### THIS CODE SUPPORTED ApproximateCC2\n",
    "\n",
    "import time\n",
    "\n",
    "def ApproximateMST2(P, c, gamma):\n",
    "    \n",
    "    E = set()\n",
    "    q = random.choice(P)\n",
    "    \n",
    "    d = len(P[0])\n",
    "    r = d\n",
    "            \n",
    "    n = len(P)\n",
    "    M = int(math.log(n / gamma, 1 + gamma))\n",
    "    \n",
    "    R = []\n",
    "    for i in range(M + 1):\n",
    "        r_i = r / math.pow(1 + gamma, M - i)\n",
    "        if r_i * c >= d:\n",
    "            R.append((d - 1) / c)\n",
    "            break\n",
    "        elif r_i >= 1: # only for Hamming\n",
    "            R.append(r_i)\n",
    "        \n",
    "    partitions = dict()\n",
    "    edge_sum = 0\n",
    "    \n",
    "    for p in P:\n",
    "        partitions[p] = {p}\n",
    "                \n",
    "    for i in range(len(R)):\n",
    "        part, E_prime = ApproximateCC2(P.copy(), partitions, c, R[i])\n",
    "        for e in E_prime:\n",
    "            \n",
    "            u, v = e\n",
    "            wt = D(u, v)\n",
    "                        \n",
    "            P_i = partitions[u]\n",
    "            P_j = partitions[v]\n",
    "                                \n",
    "            if P_i != P_j:\n",
    "                P_new = P_i.union(P_j)\n",
    "                for p in P_new:\n",
    "                    partitions[p] = P_new\n",
    "                E.add(e)\n",
    "                edge_sum += wt\n",
    "    return edge_sum, E, partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd611acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "### THIS CODE SUPPORTED ApproximateCC3\n",
    "\n",
    "import time\n",
    "\n",
    "def ApproximateMST3(P, c, gamma):\n",
    "    \n",
    "    E = set()\n",
    "    \n",
    "    d = len(P[0])\n",
    "    r = d\n",
    "            \n",
    "    n = len(P)\n",
    "    M = int(math.log(n / gamma, 1 + gamma))\n",
    "    \n",
    "    R = []\n",
    "    for i in range(M + 1):\n",
    "        r_i = r / math.pow(1 + gamma, M - i)\n",
    "        if r_i * c >= d:\n",
    "            R.append((d - 1) / c)\n",
    "            break\n",
    "        elif r_i >= 1: # only for Hamming\n",
    "            R.append(r_i)\n",
    "    R.append((d - 1) / c)\n",
    "\n",
    "        \n",
    "    partitions_list = []\n",
    "    partitions_map = dict()\n",
    "    edge_sum = 0\n",
    "    \n",
    "    for p in P:\n",
    "        partitions_list.append({p})\n",
    "        partitions_map[p] = partitions_list[-1]\n",
    "                \n",
    "    for i in range(len(R)):\n",
    "        part, E_prime = ApproximateCC3(P.copy(), partitions_list.copy(), partitions_map.copy(), c, R[i])\n",
    "\n",
    "        for e in E_prime:\n",
    "            \n",
    "            u, v = e\n",
    "            wt = D(u, v)\n",
    "                    \n",
    "            P_i = partitions_map[u]\n",
    "            P_j = partitions_map[v]\n",
    "\n",
    "                \n",
    "            if P_i != P_j:\n",
    "                new_part = P_i.union(P_j)\n",
    "                partitions_list.remove(P_i)\n",
    "                partitions_list.remove(P_j)\n",
    "                partitions_list.append(new_part)\n",
    "                for q in new_part:\n",
    "                    partitions_map[q] = new_part\n",
    "                E.add(e)\n",
    "                edge_sum += wt\n",
    "     \n",
    "    return edge_sum, E, partitions_list"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
